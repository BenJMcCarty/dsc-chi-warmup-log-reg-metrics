{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# Do you even compare the metrics of your models bro"]}, {"cell_type": "code", "execution_count": 51, "metadata": {}, "outputs": [], "source": ["\n", "#run as-is\n", "\n", "import pandas as pd\n", "\n", "from sklearn.datasets import make_classification\n", "\n", "from sklearn.preprocessing import StandardScaler\n", "\n", "from sklearn.linear_model import LogisticRegression\n", "from sklearn.metrics import confusion_matrix\n", "from sklearn.model_selection import train_test_split\n", "\n", "data = make_classification(n_samples=10000, random_state=666, n_informative=6)\n", "\n", "X = pd.DataFrame(data[0])\n", "y = data[1]\n", "\n", "data = X.copy()\n", "data['target'] = y"]}, {"cell_type": "markdown", "metadata": {}, "source": ["#### How many features in `data`?  How many classes?  Is there a class imbalance?"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["\n", "\n", "\n", "'''\n", "20\n", "\n", "2\n", "\n", "nope!\n", "'''"]}, {"cell_type": "markdown", "metadata": {}, "source": ["#### Train-test split (`random_state` = 666) and standard scale all features\n", "\n", "  - Why do we standardize *after* the train test split, and not before?\n", "\n", "  - Why do we scale the training data separately from the testing data?"]}, {"cell_type": "code", "execution_count": 16, "metadata": {}, "outputs": [], "source": ["\n", "X = data.iloc[:, :20]\n", "y = data['target']\n", "\n", "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.2, random_state=666)\n", "\n", "scaler = StandardScaler()\n", "\n", "X_train = scaler.fit_transform(X_train)\n", "X_test = scaler.fit_transform(X_test)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["#### Create a logistic regression model with the first three features of the training data (with no regularization)"]}, {"cell_type": "code", "execution_count": 42, "metadata": {}, "outputs": [{"data": {"text/plain": ["LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n", "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n", "                   multi_class='auto', n_jobs=None, penalty='none',\n", "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n", "                   warm_start=False)"]}, "execution_count": 42, "metadata": {}, "output_type": "execute_result"}], "source": ["\n", "X_train_3 = (\n", "    pd.DataFrame(X_train)\n", "    .iloc[:,:3]\n", ")\n", "\n", "log_reg = LogisticRegression(penalty='none') \n", "\n", "log_reg.fit(X_train_3, y_train)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["#### Get predictions for this 3-feature model for the training data\n", "\n", "- Assign them to `train_preds_3`"]}, {"cell_type": "code", "execution_count": 37, "metadata": {}, "outputs": [], "source": ["\n", "train_preds_3 = log_reg.predict(X_train_3)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["#### Get predictions for this 3-feature model for the testing data\n", "\n", "- Assign them to `test_preds_3`"]}, {"cell_type": "code", "execution_count": 38, "metadata": {}, "outputs": [], "source": ["\n", "X_test_3 = (\n", "    pd.DataFrame(X_test)\n", "    .iloc[:,:3]\n", "\n", ")\n", "\n", "test_preds_3 = log_reg.predict(X_test_3)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["#### Generate two confusion matrices, one each for the training predictions and testing predictions"]}, {"cell_type": "code", "execution_count": 39, "metadata": {}, "outputs": [], "source": ["\n", "train_cm = confusion_matrix(y_train, train_preds_3)\n", "test_cm = confusion_matrix(y_test, test_preds_3)\n", "\n", "print(train_cm)\n", "print(test_cm)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["#### Calculate the accuracy, recall, and precision for the training predictions\n", "\n", "#### Calculate the accuracy, recall, and precision for the testing predictions"]}, {"cell_type": "code", "execution_count": 40, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["\n", "training \n", "accuracy: 0.734875\n", "precision: 0.7315502724120851 \n", "recall: 0.7401653720871962\n", "\n", "\n", "\n", "test \n", "accuracy: 0.7305\n", "precision: 0.7272727272727273 \n", "recall: 0.7410358565737052\n", "\n"]}], "source": ["\n", "tn, fp, fn, tp = train_cm.ravel()\n", "\n", "print(f'''\n", "training \n", "accuracy: {(tn+tp)/len(X_train)}\n", "precision: {(tp)/(tp+fp)} \n", "recall: {tp/(tp+fn)}\n", "'''\n", ")\n", "print()\n", "\n", "tn, fp, fn, tp = test_cm.ravel()\n", "\n", "print(f'''\n", "test \n", "accuracy: {(tn+tp)/len(X_test)}\n", "precision: {(tp)/(tp+fp)} \n", "recall: {tp/(tp+fn)}\n", "'''\n", ")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["#### Is the model over- or under-fitting?  How can you tell?\n", "\n", "#### Is bias or variance more of a problem with this model?"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["\n", "'''\n", "Underfitting, because the train and test error are fairly close, \n", "but they're both low and can be improved\n", "\n", "Bias, because the model's performance doesn't change much when \n", "we're predicting on new data vs the data the model was trained on,\n", "but the overall performance of the model is relatively poor\n", "\n", "'''"]}, {"cell_type": "markdown", "metadata": {}, "source": ["#### Run models with the first 10 variables, then another model with all the variables\n", "  - Generate confusion matrices and calculate accuracy, precision and recall as you did above\n", "  - **BONUS**: use functions to do so!\n", "  \n", "#### How is the problem you diagnosed in the 3-variable model altered in the 10-variable and 20-variable models?\n", "\n", "#### What new problems crop up?"]}, {"cell_type": "code", "execution_count": 47, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["\n", "    training metrics for 10-variable model \n", "    accuracy: 0.934375\n", "    precision: 0.9257985257985258 \n", "    recall: 0.9441242796291657\n", "    \n", "\n", "\n", "    testing metrics for 10-variable model \n", "    accuracy: 0.9265\n", "    precision: 0.9213372664700098 \n", "    recall: 0.9332669322709163\n", "    \n", "\n", "\n", "    training metrics for 20-variable model \n", "    accuracy: 0.934125\n", "    precision: 0.925343811394892 \n", "    recall: 0.9441242796291657\n", "    \n", "\n", "\n", "    testing metrics for 20-variable model \n", "    accuracy: 0.9255\n", "    precision: 0.9195289499509323 \n", "    recall: 0.9332669322709163\n", "    \n", "\n", "\n", "Underfitting is much less of a problem for the 10-variable and 20-variable models.\n", "\n", "Accuracy, precision and recall all took huge jumps into the range above 90%.\n", "\n", "The metrics for the 10-variable model are close together for the train and\n", "test set of the 10-variable model, indicating relatively low bias and variance\n", "\n", "The metrics for the 20-variable model are slighly further apart for the \n", "train and test set, indicating that overfitting is starting to creep\n", "in slightly.\n", "\n", "\n"]}], "source": ["\n", "def make_cms(xtrain, xtest, ytrain, ytest, model_obj):\n", "    '''\n", "    returns train and test confusion matrices for a given model\n", "    '''\n", "    \n", "    model_obj.fit(xtrain, ytrain)\n", "    \n", "    preds_train = model_obj.predict(xtrain)\n", "    \n", "    preds_test = model_obj.predict(xtest)    \n", "    \n", "    cm_train = confusion_matrix(ytrain, preds_train)\n", "    \n", "    cm_test = confusion_matrix(ytest, preds_test)\n", "    \n", "    return cm_train, cm_test\n", "\n", "def cm_calcs(cm, label):\n", "    '''\n", "    calculate accuracy, precision and recall from a confusion matrix\n", "    '''\n", "    \n", "    tn, fp, fn, tp = cm.ravel()\n", "\n", "    print(f'''\n", "    {label} \n", "    accuracy: {(tn+tp)/(tn+tp+fn+fp)}\n", "    precision: {(tp)/(tp+fp)} \n", "    recall: {tp/(tp+fn)}\n", "    '''\n", "    )\n", "    print()\n", "    \n", "    return\n", "    \n", "    \n", "def model_calc(xtrain, xtest, ytrain, ytest, model_obj, train_label, test_label):\n", "    cm_train, cm_test = make_cms(xtrain, xtest, ytrain, ytest, model_obj)\n", "    \n", "    cm_calcs(cm_train, train_label)\n", "    cm_calcs(cm_test, test_label)\n", "    \n", "    return\n", "\n", "#10-feature model\n", "X_train_10 = pd.DataFrame(X_train).iloc[:,:10]\n", "X_test_10 = pd.DataFrame(X_test).iloc[:,:10]\n", "\n", "model_calc(X_train_10, X_test_10, y_train, y_test, \n", "           LogisticRegression(penalty='none'), \n", "          'training metrics for 10-variable model',\n", "          'testing metrics for 10-variable model')\n", "\n", "#20-feature model\n", "model_calc(X_train, X_test, y_train, y_test,\n", "           LogisticRegression(penalty='none'), \n", "          'training metrics for 20-variable model',\n", "          'testing metrics for 20-variable model')\n", "\n", "print('''\n", "Underfitting is much less of a problem for the 10-variable and 20-variable models.\n", "\n", "Accuracy, precision and recall all took huge jumps into the range above 90%.\n", "\n", "The metrics for the 10-variable model are close together for the train and\n", "test set of the 10-variable model, indicating relatively low bias and variance\n", "\n", "The metrics for the 20-variable model are slighly further apart for the \n", "train and test set, indicating that overfitting is starting to creep\n", "in slightly.\n", "\n", "''')"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.7.4"}}, "nbformat": 4, "nbformat_minor": 4}